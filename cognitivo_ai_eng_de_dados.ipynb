{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cognitio.ai_eng_de_dados.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9jSDiEOQYPx"
      },
      "source": [
        "# Cognitivo.AI - Atividade Técnica\n",
        "<sub>*Engenheiro de Dados*</sub>\n",
        "\n",
        "**Para realizar essa atividade utilizei o Google Colab, por ser um ambiente em nuvem que provê uso de GPU do Google para processamento de dados e que já contêm diversas bibliotecas instaladas.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlJkFhIyR1uC"
      },
      "source": [
        "Para trabalhar com spark no Google Colab é preciso realizar alguns procedimentos instalando as dependencias e configurando as variáveis de ambientes, conforme os procedimentos abaixo descrevem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcUoki6WMeD6"
      },
      "source": [
        "# instalando as dependencias necessárias para usar o pyspark no google colab\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48dc2qlbMuJe"
      },
      "source": [
        "# configuração das variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        " \n",
        "# sequencia para fazer o pyspark ser \"importável\" para o google colab\n",
        "import findspark\n",
        "findspark.init('spark-3.0.1-bin-hadoop3.2')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWH3tpeKM6ol",
        "outputId": "f857ca72-079f-4f12-fb79-e8d9ed353794"
      },
      "source": [
        "# iniciar uma sessão local e importar dados\n",
        "from pyspark.sql import SparkSession, Row\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        " \n",
        "# download do http para arquivo local\n",
        "!wget --quiet --show-progress https://raw.githubusercontent.com/zorrex82/cognitivo_ai_eng_dados/master/load.csv\n",
        " \n",
        "# carregando os dados\n",
        "df_spark = sc.read.csv(\"./load.csv\", inferSchema=True, header=True)\n",
        " \n",
        "# ver algumas informações sobre os tipos de dados de cada coluna\n",
        "df_spark.printSchema()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rload.csv.5            0%[                    ]       0  --.-KB/s               \rload.csv.5          100%[===================>]   1.00K  --.-KB/s    in 0s      \n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- phone: string (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- create_date: string (nullable = true)\n",
            " |-- update_date: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcvl1ATyyHux"
      },
      "source": [
        "Processo de conversão de um dataframe em um arquivo Parquet, optei por usar o Apache Parquet pelos motivos:\n",
        "\n",
        "1.   Armazenamento Eficiente (a compressão por colunas é mais eficaz, sendo que em cenários de big data optimizar o armazenamento diminui os custos que por vezes pode invalidar ou viabilizar um projeto.\n",
        "2.   Algoritmo de compressão que pode ser espeficicado por coluna.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viHZ_SI6QOHM",
        "outputId": "a0199741-a720-4502-e39c-1ef5d85198e3"
      },
      "source": [
        "# Primeiro salvei o dataframe em um arquivo parquet\n",
        "df_spark.write.parquet(\"load.parquet\")\n",
        "\n",
        "# Carreguei o arquivo Parquet criado para realizar algumas consultas e verificar que todas as informações estão preservadas.\n",
        "parquet_df = sc.read.parquet(\"load.parquet\")\n",
        "\n",
        "# Então eu crio uma View temporária para executar alguns comandos SQL.\n",
        "parquet_df.createOrReplaceTempView(\"loadView\")\n",
        "\n",
        "# Listei todos os itens salvos no Parquet para conferir com o dataframe inicial.\n",
        "load_view = sc.sql(\"SELECT * FROM loadView order by id\")\n",
        "load_view.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
            "| id|                name|               email|          phone|             address|age|         create_date|         update_date|\n",
            "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
            "|  1|david.lynch@cogni...|         David Lynch|(11) 99999-9998|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-04-14 17:09:...|\n",
            "|  1|david.lynch@cogni...|         David Lynch|(11) 99999-9997|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-03-03 18:47:...|\n",
            "|  1|david.lynch@cogni...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-05-23 10:13:...|\n",
            "|  2|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|2018-04-21 20:21:...|\n",
            "|  3|spongebob.squarep...|Spongebob Squarep...|(11) 91234-5678|124 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 04:07:...|\n",
            "|  3|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 05:08:...|\n",
            "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0xHUnrOuTWQ"
      },
      "source": [
        "Processo de deduplicação dos dados contidos no arquivo.\n",
        "Primeiro fiz uma breve investigação da coluna update_date e em seguida optei por usar as functions do PySpark para selecionar a coluna e remover as entradas duplicadas mantendo apenas a mais recente atualizada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdnUVjI4RbdI",
        "outputId": "e6ba5227-03c4-44e1-dc84-2107944c5915"
      },
      "source": [
        "# Investigando melhor a coluna update_date que será usada para a deduplicação dos dados\n",
        "load_view.describe(['update_date']).show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|summary|         update_date|\n",
            "+-------+--------------------+\n",
            "|  count|                   6|\n",
            "|   mean|                null|\n",
            "| stddev|                null|\n",
            "|    min|2018-03-03 18:47:...|\n",
            "|    max|2018-05-23 10:13:...|\n",
            "+-------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW_YyeTDhqGR",
        "outputId": "03d78575-3aca-427f-bb09-156f40a3e7f1"
      },
      "source": [
        "# Importando as funções do Pyspark e removendo os dados duplicados com base nos criterios pré estabelecidos\n",
        "from pyspark.sql.functions import * \n",
        "\n",
        "df = load_view.orderBy(col('update_date').desc()).dropDuplicates(['id'])\n",
        "df.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
            "| id|                name|               email|          phone|             address|age|         create_date|         update_date|\n",
            "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
            "|  1|david.lynch@cogni...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-05-23 10:13:...|\n",
            "|  3|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 05:08:...|\n",
            "|  2|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|2018-04-21 20:21:...|\n",
            "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyQfiy24Yoj"
      },
      "source": [
        "*Fiz um map do arquivo json para utilizar na formatação do resultado usando as funções read.format do spark passando o arquivo json como option e por fim carregando o arquivo csv com os campos selecionados.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JBQ1Lk0JWF9"
      },
      "source": [
        "# Importando as bibliotecas necessarias\n",
        "import json\n",
        "import urllib.request\n",
        "\n",
        "# Carregando o arquivo json\n",
        "url = \"https://raw.githubusercontent.com/zorrex82/cognitivo_ai_eng_dados/master/types_mapping.json\"\n",
        "\n",
        "json_url = urllib.request.urlopen(url)\n",
        "\n",
        "# Criando o map do pipeline\n",
        "pipeline = json.loads(json_url.read())\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ogRYPxwFeQ"
      },
      "source": [
        "# Criei uma variavel para conter os nomes das colunas desejadas\n",
        "colunas = ['age', 'create_date', 'update_date']\n",
        "\n",
        "# Gerei um novo dataframe somente com os campos informados\n",
        "new_df = df[colunas]\n",
        "\n",
        "# Novo arquivo csv para gerar um arquivo final\n",
        "new_df.write.csv(\"new_load.csv\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkH5VMIEw60-"
      },
      "source": [
        "# Gerendo o arquivo com as transformações dos campos.\n",
        "df = sc.read.format('com.databricks.spark.csv').option(\"pipeline\", pipeline).load(\"new_load.csv\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSd3fwkr6UiR",
        "outputId": "95ca1104-a705-4f89-d817-40528af7f50e"
      },
      "source": [
        "df.toDF(*colunas).printSchema()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: string (nullable = true)\n",
            " |-- create_date: string (nullable = true)\n",
            " |-- update_date: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}